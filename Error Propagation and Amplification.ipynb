{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "2.6 Error Propagation and Amplification"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Errors can be amplified as they *propagate* through a computation. For example, suppose the divided difference (2.3) is part of a long calculation:"
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "   .    .\n",
      "f1 = . . . ; \\\\ approx of f(x)\n",
      "f2 = . . . ; \\\\ approx of f(x+h)\n",
      "   .    .\n",
      "fPrimeHat = (f2 - f1) / h; \\\\ approx of derivative"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "It is unlikely that $f_{1} = \\widehat{f(x)} \\approx f(x)$  is exact. Many factors may contribute to the errors $e_{1} = f_{1} - f(x)$ and $e_{2} = f_{2} - f(x + h)$. There are three contributions to the final error in $f^{\\prime}$:\n",
      "\n",
      "$\\widehat{f^{\\prime}} - f^{\\prime} = e_{r} + e_{tr} + e_{pr}$ (2.4)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "One is the roundoff error in evaluating $( f_{2} - f_{1} ) / h$ in floating point\n",
      "\n",
      "$\\widehat{f^{\\prime}} = \\frac{f_{2} - f_{1}}{h} + e_{r}$ (2.5)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The truncation error in the difference quotient approximation is\n",
      "\n",
      "$\\frac{f(x + h) - f(x)}{h} - f^{\\prime} = + e_{tr}$ (2.6)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The *propagated error* comes from using inexact values of $f(x + h)$ and $f(x)$:\n",
      "\n",
      "$\\frac{f_{2} - f_{1}}{h} - \\frac{f(x + h) - f(x)}{h} = \\frac{e_{2} - e_{1}}{h} = e_{pr}$ (2.7)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "If we add (2.5), (2.6), and (2.7), and simplify, we get the formula (2.4)."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "A stage of a long calculation creates some errors and propagates errors from\n",
      "earlier stages, possibly with amplification. In this example, the difference\n",
      "quotient evaluation introduces truncation and roundoff error. Also, $e_{1}$ and $e_{2}$\n",
      "represent errors generated at earlier stages when $f(x)$and $f(x + h)$ were evaluated.\n",
      "These errors, in turn, could have several sources, including inaccurate x values\n",
      "and roundoff in the code evaluating $f(x)$. According to (2.7), the difference\n",
      "quotient propagates these and amplifies them by a factor of $1/h$. A typical\n",
      "value $h = .01$ could amplify incoming errors $e_{1}$ and $e_{2}$ by a factor of 100."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This increase in error by a large factor in one step is an example of catastrophic cancellation. If the numbers $f(x)$ and $f (xh)$ are nearly equal, the difference can have much less relative accuracy than the numbers themselves. More common and more subtle is gradual error growth over a long sequence of computational steps. Exercise 2.12 has an example in which the error roughly doubles at each stage. Starting from double precision roundoff level, the error after 30 steps is negligible but the error after 60 steps is larger than the answer."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "An algorithm in *unstable* if its error mainly comes from amplification. This *numerical instability* can be hard to discover by standard debugging techniques that look for the first place that something goes wrong, particularly if there is gradual error growth."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Mathematical *stability theory* in scientific computing is the search for gradual error growth in computational algorithms. It focuses on propagated error only, ignoring the original sources of error. For example, Exercise 8 involves the backward recurrence $f_{k-1} = f_{k+1} - f_{k}$. In a stability analysis, we would assume that the subtraction is performed exactly and that the error in $f_{k\u22121}$ is entirely due to errors in $f_{k}$ and $f_{k + 1}$ . That is, if $f_{k} = f_{k} + e_{k}$ is the computer approximation, then the $e_{k}$ satisfy the mathematical relation $e_{k\u22121} = e_{k+1} \u2212 e_{k}$ , which is the error propagation equation. We then would use the theory of recurrence relations to see whether the $e_{k}$ can grow relative to the $f_{k}$ as $k$ decreases. If this error growth is possible, it will happen in practically any computation."
     ]
    }
   ],
   "metadata": {}
  }
 ]
}