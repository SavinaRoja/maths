{
 "metadata": {
  "name": "Sources of Error"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "This notebook was made from the coursework here: http://www.cs.nyu.edu/courses/fall06/G22.2112-001/\n",
      "I am creating these notebooks for my own use as I digest the material. All intellectual property rights of Prof. Marsha Berger are to be upheld.\n",
      "Any additions of my own will be generally take the form of python code, raw text or comments, and I'll try to make them clear."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Chapter 2"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Sources of Error"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In scientific computing, we never expect to get the exact answer. Inexactness\n",
      "is practically the definition of scientific computing. Getting the exact answer,\n",
      "generally with integers or rational numbers, is symbolic computing, an interesting\n",
      "but distinct subject. Suppose we are trying to compute the number $A$. The\n",
      "computer will produce an approximation, which we call $\\hat{A}$. This $\\hat{A}$ may agree\n",
      "with $A$ to 16 decimal places, but the identity $$A = \\hat{A}$$ (almost) never is true in\n",
      "the mathematical sense, if only because the computer does not have an exact\n",
      "representation for A. For example, if we need to find x that satisfies the equation\n",
      "$x^{2} \u2212 175 = 0$, we might get 13 or 13.22876, depending on the computational method, but \n",
      "\\sqrt{175}$ cannot be represented exactly as a floating point number."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Four primary sources of error are: (i) *roundoff error*, (ii) *truncation error*,\n",
      "(iii) *termination of iterations*, and (iv) *statistical error* in Monte Carlo. We\n",
      "will estimate the sizes of these errors, either *a priori* from what we know in\n",
      "advance about the solution, or *a posteriori* from the computed (approximate)\n",
      "solutions themselves. Software development requires distinguishing these errors\n",
      "from those caused by outright bugs. In fact, the bug may not be that a formula\n",
      "is wrong in a mathematical sense, but that an approximation is not accurate\n",
      "enough. This chapter discuss floating point computer arithmetic and the IEEE\n",
      "floating point standard. The others are treated later."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Scientific computing is shaped by the fact that nothing is exact. A\n",
      "mathematical formula that would give the exact answer with exact inputs might not\n",
      "be robust enough to give an approximate answer with (inevitably) approximate\n",
      "inputs. Individual errors that were small at the source might combine and grow\n",
      "in the steps of a long computation. Such a method is *unstable*. A problem is\n",
      "*ill conditioned* if any computational method for it is unstable. *Stability* theory,\n",
      "which is modeling and analysis of error growth, is an important part of scientific\n",
      "computing."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "2.1\n",
      "Relative error, absolute error, and cancellation"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The absolute error in approximating $A$ by $\\hat{A}$ is $e = \\hat{A} \u2212 A$. The relative error,\n",
      "which is $\\epsilon = e/A$, is usually more meaningful. These definitions may be restated\n",
      "as (Eqn. 2.1)\n",
      "\n",
      "$$\\hat{A} = A + e \\text{ (absolute error)}$$\n",
      "\n",
      "$$\\hat{A} = A \u00b7 (1 + \\epsilon) \\text{ (relative error)}$$\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "For example, the absolute error in approximating $A = \\sqrt{175}$ by $\\hat{A} = 13$ is $e =\n",
      "13.22876 \u00b7 \u00b7 \u00b7 \u2212 13 \u2248 .23$. The corresponding relative error is $e/A \u2248 .23/13.2 \u2248\n",
      ".017 < 2\\%$. Saying that the error is less than 2% is probably more informative\n",
      "than saying that the error is less than .25 = 1/4."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Relative error is a *dimensionless* measure of error. In practical situations,\n",
      "the desired A probably has units, such as seconds, meters, etc. If A is a length\n",
      "measured in meters, knowing $e \u2248 .23$ does not tell you whether $e$ is large or\n",
      "small. If the correct length is half a meter, then .23 is a large error. If the\n",
      "correct length in meters is $13.22876 \u00b7 \u00b7 \u00b7$, then A is off by less than 2%. If we\n",
      "switch to centimeters the error becomes 22.9. This may seem larger, but it still\n",
      "is less than 2% of the exact length, $1, 322.876 \u00b7 \u00b7 \u00b7 $(in centimeters)."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We often describe the accuracy of an approximation or measurement by\n",
      "saying how many decimal digits are correct. For example, Avogadro\u2019s number\n",
      "(the number of molecules in one mole) with two digits of accuracy is $N_{0} \u2248\n",
      "6.0 \u00d7 10^{23}$ . We write $6.0$ instead of just $6$ to indicate that we believe the $0$ is\n",
      "correct, that the true Avogadro\u2019s number is closer to $6 \u00d7 10^{23}$ than to $6.1 \u00d7 10^{23}$\n",
      "or $5.9 \u00d7 10^{23}$ . With three digits the number is $N_{0} \u2248 6.02 \u00d7 10^{23}$ . In an absolute\n",
      "sense, the difference between $N_{0} \u2248 6 \u00d7 10^{23}$ and $N_{0} \u2248 6.02 \u00d7 10^{23}$ is $2 \u00d7 10^{21}$\n",
      "molecules per mole, which may seem like a lot, but the relative error is about a\n",
      "third of one percent."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "While relative error is more useful than absolute error, it also is more problematic.\n",
      "Relative error can grow through *cancellation*. For example, suppose\n",
      "we know $A = B \u2212 C$ and we have evaluated $B$ and $C$ to three decimal digits\n",
      "of accuracy. If the first two digits of $B$ and $C$ agree, then they cancel in the\n",
      "subtraction, leaving only one correct digit in $A$. If, say, $B \u2248 \\hat{B} = 2.38 \u00d7 10^{5}$\n",
      "and $C \u2248 \\hat{C} = 2.33 \u00d7 10^{5}$, then $A \u2248 \\hat{A} = 5 \u00d7 10^{3}$ . This $A$ is probably off by more\n",
      "than $10\\%$ even though $B$ and $C$ had relative error less than $1\\%$. *Catastrophic\n",
      "cancellation* is losing many digits in one subtraction. More subtle and more\n",
      "common is an accumulation of less dramatic cancellations over a series of steps."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "2.2 Computer arithmetic"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Error from inexact computer floating point arithmetic is called *roundoff error*.\n",
      "Roundoff error occurs in most floating point operations. Some computations \n",
      "involve no other approximations. For example, solving systems of linear equations\n",
      "using Gaussian elimination would give the exact answer in *exact arithmetic* (all\n",
      "computations performed exactly). Even these computations can be unstable\n",
      "and give wrong answers. Being exactly right in exact arithmetic does not imply\n",
      "being approximately right in floating point arithmetic."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Floating point arithmetic on modern computers is governed by the *IEEE\n",
      "floating point standard*. Following the standard, a floating point operation normally\n",
      "has relative error less than the *machine precision*, but of the same order\n",
      "of magnitude. The machine precision is $\\epsilon_{mach} \u2248 6 \u00b7 10^{-8}$ for single precision\n",
      "(data type `float` in C), and $\\epsilon_{mach} = 2^{-53} \u2248 10^{-16}$ for double precision (data\n",
      "type `double` in C). Let $A = B \\bigcirc C$, with $\\bigcirc$\n",
      "standing for one of the arithmetic\n",
      "operations: addition ($A = B+C$), subtraction, multiplication, or division. With\n",
      "the same $B$ and $C$, the computer will produce $\\hat{A}$ with relative error (2.1) that\n",
      "normally satisfies $|\\epsilon| \u2264 \\epsilon_{mach}$."
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "2.2.1 Introducing the standard"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The *IEEE floating point standard* is a set of conventions for computer\n",
      "representation and processing of floating point numbers. Modern computers follow\n",
      "these standards for the most part. The standard has four main goals:\n",
      "\n",
      "  1. To make floating point arithmetic as accurate as possible.\n",
      "  2. To produce sensible outcomes in exceptional situations.\n",
      "  3. To standardize floating point operations across computers.\n",
      "  4. To give the programmer control over exception handling."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The standard specifies exactly how numbers are represented in hardware.\n",
      "The most basic unit of information that a computer stores is a *bit*, a variable\n",
      "whose value may be either 0 or 1. Bits are organized into 32 bit or 64 bit *words*,\n",
      "or *bit strings*. The number of 32 bit words is $^{1}$ $2^{32} = 2^{2} \u00b7 2^{30} \u2248 4 \u00d7 (10^{3})^{3} = 4\n",
      "\\text{ billion} $. A typical computer should take well under a minute to list all of them.\n",
      "A computer running at 1GHz in theory can perform one billion operations per\n",
      "second, though that may not be achieved in practice. The number of 64 bit\n",
      "words is about $1.6 \u00b7 10^{19}$ , which is too many to be listed in a year. A 32 bit\n",
      "floating point number is called *single precision* and has data type `float` in\n",
      "C/++. A 64 bit floating point number is called *double precision* and has data\n",
      "type `double`.\n",
      "\n",
      "*$^{1}$We use the approximation $2^{10} = 1024 \u2248 10^{3}$.*"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#PJB: A simple calculation will tell us the number of representable words with single and double precision.\n",
      "2 ** 32 - 1, 2 ** 64 - 1"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "C/C++ also has data types `int` (for 32 bits) and `longint` (for 64 bits) that\n",
      "represent integers. Integer, or fixed point arithmetic, is very simple. With 32\n",
      "bit integers, the $2^{32} \u2248 4 \u00b7 10^{9}$ distinct words represent that many consecutive\n",
      "integers, filling the range from about $\u22122 \u00b7 10^{9}$ to about $2 \u00b7 10^{9}$ . Addition,\n",
      "subtraction, and multiplication are done exactly whenever the answer is within this\n",
      "range. The result is unpredictable when the answer is out of range (*overflow*).\n",
      "Results of integer division are rounded down to the nearest integer below the\n",
      "answer."
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "2.2.2 Representation of numbers, arithmetic operations"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "For scientific computing, integer arithmetic has two drawbacks. One is that\n",
      "there is no representation for numbers that are not integers. Also important\n",
      "is the small range of values. The number of dollars in the US national debt,\n",
      "several trillion (10^{12}), cannot be represented as a 32 bit integer but is easy to\n",
      "approximate in 32 bit floating point."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The standard assigns a real number value to each single precision or double\n",
      "precision bit string. On a calculator display, the expression:\n",
      "\n",
      "$$\u2212.2491E \u2212 5$$\n",
      "\n",
      "means $-2.491 \u00b7 10^{-6}$ . This expression consists of a sign bit, $s = -$, a mantissa,\n",
      "$m = 2491$ and an exponent, $e = -5$. The expression $s.mEe$ corresponds to the\n",
      "number $s\u00b7.m\u00b710^{e}$ . Scientists like to put the first digit of the mantissa on the left\n",
      "of the decimal point ($-2.491 \u00b710^{-6}$) while calculators put the whole thing on the\n",
      "right ($-.2491 \u00b7 10^{-5}$). In base 2 (binary) arithmetic, the scientists\u2019 convention\n",
      "saves a bit, see below."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "When the standard interprets a 32 bit word, the first bit is the sign bit, $s = \u00b1$.\n",
      "The next 8 bits form the $exponent^{2}$, $e$, and the remaining 23 bits determine the\n",
      "form the $fraction$, $f$. There are two possible signs, $2^{8} = 256$ possible values of\n",
      "$e$(ranging from 0 to 255), and $2^{23} \u2248 8 \\text{ million}$ possible fractions. Normally a\n",
      "floating point number has the value (Eqn. 2.2)\n",
      "\n",
      "$$A = \u00b12^{e-127} \u00b7 (1.f)_{2}$$\n",
      "\n",
      "where $f$ is base 2 and the notation $(1.f)_{2}$ means that the expression $1.f$ is\n",
      "interpreted in base 2. Note that the mantissa is $1.f$ rather than just the fractional\n",
      "part, $f$ . Any number (except $0$) can be normalized so that its base 2 mantissa\n",
      "has the form $1.f$. There is no need to store the \"1.\" explicitly, which saves one\n",
      "bit.\n",
      "\n",
      "For example, the number $2.752 \u00b7 10^{3} = 2572$ can be written\n",
      "\n",
      "\\begin{align}\n",
      "2752 &= 2^{11} + 2^{9} + 2^{7} + 2^{6} \\\\\n",
      "&= 2^{11} \u00b7 (1 + 2^{-2} + 2^{-4} + 2^{-5}) \\\\\n",
      "&= 2^{11} \u00b7 (1 + (.01)_{2} + (.0001)_{2} + (.00001)_{2} ) \\\\\n",
      "&= 2^{11} \u00b7 (1.01011)_{2} .\n",
      "\\end{align}\n",
      "\n",
      "Altogether, we have, using $11 = (1011)_{2}$ ,\n",
      "\n",
      "$$2752 = +(1.01011)_{2}^{(1011)_{2}}$$.\n",
      "\n",
      "\n",
      "Thus, we have sign $s = +$. The exponent is $e \u2212 127 = 11$ so that $e = 138 =\n",
      "(10001010)_{2}$ . The fraction is $f = (01011000000000000000000)_{2}$ . The entire 32\n",
      "bit string corresponding to $2.752 \u00b7 10^{3}$ then is:\n",
      "\n",
      "$$\\underbrace{1}_{\\text{s}} \\underbrace{10001010}_{\\text{e}} \\underbrace{01011000000000000000000}_{\\text{f}}$$\n",
      "\n",
      "*$^{2}$This is a slight misnomer; the actual exponent is $e - 127$ (in single precision) exponent.*"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "For arithmetic operations, the standard mandates the rule: *the exact answer,\n",
      "correctly rounded*. For example, suppose `x`, `y`, and `z` are computer variables of\n",
      "type `float`, and the computer executes the statement `x = y / z;`. Let $B$ and $C$\n",
      "be the numbers that correspond to the 32 bit strings `y` and `z` using the standard\n",
      "(2.2). A number that can be represented exactly in form (2.2) using 32 bits is a\n",
      "(32 bit) *floating point number*. Clearly $B$ and $C$ are floating point numbers, but\n",
      "the exact quotient, $\\hat{A} = B/C$, probably is not. *Correct rounding* means finding\n",
      "the floating point number $\\hat{A}$ closest $^{3}$ to $A$. The computer is supposed to set the\n",
      "bit string `x` equal to the bit string representing $\\hat{A}$. For exceptions to this rule, see below.\n",
      "\n",
      "*$^{3}$Ties can happen. The accuracy of IEEE floating point arithmetic does not depend on\n",
      "how ties are resolved.*"
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "TODO: Finish converting sections 2.2 and 2.3"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "2.4 Iterative Methods"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Suppose we want to find $A$ by solving an equation. Often it is impossible to\n",
      "find a formula for the solution. Instead, *iterative* methods construct a sequence\n",
      "of approximate solutions, $A_{n}$ , for $n = 1, 2, . . . .$ Hopefully, the approximations\n",
      "*converge* to the right answer: $A_{n} \u2192 A$ as $n \u2192 \u221e$. In practice, we must stop the\n",
      "iteration process for some large but finite $n$ and accept $A_{n}$ as the approximate\n",
      "answer."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "For example, suppose we have a $y > 0$ and we want to find $x$ with $xe^{x} = y$.\n",
      "There is not a formula for $x$, but we can write a program to carry out the\n",
      "iteration: $x_{1} = 1, x_{n+1} = ln(y)\u2212ln(x_{n})$. The numbers $x_{n}$ are iterates. The limit\n",
      "$x = \\lim_{n \\to \\infty} x_{n}$ (if it exists), is a fixed point of the iteration, i.e. $x = ln(y)\u2212ln(x)$,\n",
      "which implies $xe^{x} = y$. Figure 2.4 demonstrates the convergence of the iterates\n",
      "in this case with y = 10. The initial guess is x1 = 1. After 20 iterations, we\n",
      "have $x_{20} \u2248 1.74$. The error is $e_{20} \u2248 2.3 \u00b7 10^{-5} $, which might be small enough,\n",
      "depending on the application."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "After 67 iterations the relative error is $(x_{67} - x)/x \u2248 2.2 \u00b7 10^{-16} /1.75 \u2248\n",
      "1.2 \u00b7 10^{-16}$, which is only slightly larger than double precision machine precision\n",
      "$\\epsilon_{mach} \u2248 1.1 \u00b7 10^{-16}$ . This shows that supposedly approximate iterative methods\n",
      "can be as accurate as *direct* methods that would be exact in exact arithmetic.\n",
      "It would be a surprising fluke for even a direct method to achieve better than\n",
      "machine precision because even they are subject to roundoff error."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\\begin{table}\n",
      " \\begin{tabular}{c0 | c1 | c2 | c3 | c4 | c5 | c6}\n",
      "  n & 1 & 3 & 6 & 10 & 20 & 67 \\\\\n",
      "  x_{n} & 1 & 1.46 & 1.80 & 1.751 & 1.74555 & 1.745528 \\\\\n",
      "  \\epsilon_{n} & -.745 & -.277 & 5.5 \u00b7 10^{-2} & 5.9 \u00b7 10^{-3} & 23 \u00b7 10^{-5} & 2.2 \u00b7 10^{-16}\n",
      "\\end{tabular}\n",
      "\\end{table}"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}