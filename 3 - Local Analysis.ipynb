{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "This notebook was made from the coursework here: http://www.cs.nyu.edu/courses/fall06/G22.2112-001/\n",
      "I am creating these notebooks for my own use as I digest the material. All intellectual property rights of Prof. Marsha Berger are to be upheld.\n",
      "Any additions of my own will be generally take the form of python code, raw text or comments, and I'll try to make them clear."
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Chapter 3"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Local Analysis"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Among the most common computational tasks are differentiation, interpolation, and integration. The simplest methods used for these operations are *finite difference* approximations for derivatives, *low order polynomial* interpolation, and *panel method* integration. Finite difference formulas, integration rules, and interpolation form the core of most scientific computing projects that involve\n",
      "solving differential or integral equations.\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The finite difference formulas (3.14) range from simple low order approximations (3.14a) \u2013 (3.14c) to not terribly complicated high order methods such as\n",
      "(3.14e). Figure 3.2 illustrates that high order methods can be far more accurate\n",
      "than low order ones. This can make the difference between getting useful answers and not in serious large scale applications. The methods here will enable\n",
      "the reader to design professional quality highly accurate methods rather than\n",
      "relying on simple but often inefficient low order ones.\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Many methods for these problems involve a *step size*, $h$. For each $h$ there is an approximation $^{1}$$\\widehat{A}(h) \\approx A$. We say that $\\widehat{A}$ is *consistent* if $\\widehat{A}(h) \\rightarrow A$ as $h \\rightarrow 0$. For example, we might estimate $A = f^{\\prime}(x)$ using the finite difference formula (3.14a): $\\widehat{A}(h) = (f(x + h) - f(x)) / h$. This is consistent, as $\\text{lim}_{h \\rightarrow 0}\\widehat{A}(h)$ is the definition of $f^{\\prime}(x)$. The accuracy of the approximation depends on $f$, but the *order of accuracy* does not.$^{2}$ The approximation is *first order accurate* if the error is nearly proportional to $h$ for small enought $h$. It is *second order* if the error goes like $h^{2}$ When $h$ is small, $h^{2} << h$, so approximations with a higher order of accuracy can be much more accurate."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The design of difference formulas and integration rules is based on *local analysis*, approximations to a function *f* about a *base point* $x$. These approximations consist of the first few terms of the *Taylor series* expansion of $f$ about $x$. The *first order* approximation is\n",
      "\n",
      "$f(x + h) \\approx f(x) + hf^{\\prime}(x)$ . $\\hspace{2em}$ (3.1)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The *second order* approximation is more complicated and more accurate:\n",
      "\n",
      "$f(x + h) \\approx f(x) + f^{\\prime}(x)h + \\frac{1}{2}f^{\\prime \\prime}(x)h^{2}$ . $\\hspace{2em}$ (3.2)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Figure 3.1 illustrates the first and second order approximations. *Truncation error* is the difference between $f(c + h)$ and one of these approximations. For example, the truncation error for the first order approximation is \n",
      "\n",
      "$f(x) + f^{\\prime}(x)h - f(x + h)$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To see how Taylor series are used, substitute the approximation (3.1) into the finite difference formula (3.14a). We find\n",
      "\n",
      "$\\widehat{A}(h) \\approx f^{\\prime}(x) = A$ . $\\hspace{2em}$ (3.3)"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Figure 3.1 (Made with Python SymPy/Numpy/Matplotlib)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib inline\n",
      "\n",
      "#As the first code cell in this notebook, let's do some importing\n",
      "from IPython.display import display\n",
      "import numpy as np\n",
      "import scipy as sp\n",
      "from sympy import *\n",
      "from sympy.utilities.lambdify import lambdify\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "init_printing()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Let's make Figure 3.1\n",
      "#Imma cheat a little here, and use the true point derivatives instead of the\n",
      "#finite difference approximation, that will be covered later\n",
      "#Another thing to be aware of is SymPy's built-in series method for expressions\n",
      "x, h = symbols('x h')\n",
      "f_of_x = x * exp(2 * x)\n",
      "f_of_xp = diff(f_of_x)\n",
      "f_of_xpp = diff(diff(f_of_x))\n",
      "\n",
      "x0 = 0.6  # the base point\n",
      "h_range = (-0.3, 0.3)  # 2-tuple for range of h from base-point\n",
      "npts=100  # number of points in h_range, h size is then (h_range[1] - h_range[0]) / npts\n",
      "h_space = np.linspace(*h_range, num=npts)\n",
      "x_space = np.linspace(*map(lambda x: x0 + x, h_range), num=npts)\n",
      "\n",
      "fx0 = f_of_x.subs(x,x0).evalf()  # f(x0)\n",
      "fx0p = f_of_xp.subs(x,x0).evalf()  # f'(x0)\n",
      "fx0pp = f_of_xpp.subs(x,x0).evalf()  # f''(x0)\n",
      "\n",
      "f_of_x_solve = lambdify(x, f_of_x, 'numpy')(x_space)\n",
      "#functions of x where x = x0 + h, so they are f(h) centered on x0\n",
      "zeroth_solve = lambdify(x, fx0, 'numpy')(h_space)\n",
      "first_solve = lambdify(x, fx0 + fx0p * x, 'numpy')(h_space)\n",
      "second_solve = lambdify(x, fx0 + fx0p * x + 0.5 * fx0pp * x ** 2, 'numpy')(h_space)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plt.rc('figure', figsize=(12, 8))\n",
      "plt.title('f(x) and approximations up to second order')\n",
      "plt.xlabel('x')\n",
      "plt.ylabel('f')\n",
      "plt.grid()\n",
      "plt.xlim(map(lambda x: x0 + x, h_range))\n",
      "plt.plot(x_space, f_of_x_solve, label='f', lw=2)\n",
      "#Workaround from odd effect of lambdification of zeroth order approximation\n",
      "plt.plot(x_space, np.array([zeroth_solve] * npts), label='constant', linestyle='-.', lw=3)\n",
      "plt.plot(x_space, first_solve, label='linear', linestyle=':', lw=3)\n",
      "plt.plot(x_space, second_solve, label='quadratic', linestyle='--', lw=3)\n",
      "plt.legend(loc='best')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "*Figure 3.1: Plot of $f(x) = xe^{2x}$* together with Taylor series approximations of order zero, one, and two. The base point is $x = .6$ and h ranges from $\u2212.3$ to $.3.$ The symbols at the ends of the curves illustrate convergence at fixed $h$ as the order increases. Also, the higher order curves make closer contact with $f(x)$ as $h \\rightarrow 0$."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The more accurate Taylor approximation (3.2) allows us to estimate the error in (3.14a). Substituting (3.2) into (3.14a) gives\n",
      "\n",
      "$\\widehat{A}(h) \\approx A + A_{1}h$ , $A_{1} = \\frac{1}{2}f^{\\prime \\prime}(x)$ . $\\hspace{2em}$ (3.4)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This *asymptotic error expansion* is an estimate of $\\widehat{A} - A$ for a given $f$ and $h$. It shows that the error is roughly proportional to $h$ for small $h$. This understanding of truncation error leads to more sophisticated computational strategies. *Richardson extrapolation* combines $\\widehat{A}(h)$ and $\\widehat{A}(2h)$ to create higher order estimates with much less error. *Adaptive methods* take a desired accuracy, $e$, and attempt (without knowing $A$) to find an $h$ with $|\\widehat{A}(h) \u2212 A| \\le e$. Error expansions like (3.4) are the basis of many adaptive methods.\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This chapter focuses on truncation error and mostly ignores roundoff. In most practical computations that have truncation error, including numerical solution of differential equations or integral equations, the truncation error is much larger than roundoff. Referring to Figure 2.3, the practical range of $h$ from $.01$ to $10^{\u22125}$ , the computed error is roughly $4.1 \\cdot h$, as (3.4) suggests is should be. This starts to break down for the impractically small $h = 10^{\u22128}$. More sophisticated high order approximations reach roundoff sooner, which can be an issue in code testing, but rarely in large scale production runs."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "3.1 Taylor series and asymptotic expansions"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The Taylor series expansion of a function $f$ about the point $x$ is\n",
      "\n",
      "$f(x + h) = \\sum\\limits_{n=0}^{\\infty}\\frac{1}{n!}f^{(n)}(x)h^{n}$ . $\\hspace{2em}$ (3.5)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The notation $f^{(n)}(x)$ refers to the $n^{th}$ derivative of $f$ evaluated at $x$. The *partial sum* of order $p$ is a degree $p$ polynomial in $h$:\n",
      "\n",
      "$F_{p}(x,h) = \\sum\\limits_{n=0}^{p}\\frac{1}{n!}f^{(n)}(x)h^{n}$ . $\\hspace{2em}$ (3.6)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The partial sum $F_{p}$ is the Taylor approximation to $f(x + h)$ of order $p$. It is a polynomial of order $p$ in the variable $h$. Increasing $p$ makes the approximation more complicated and more accurate. The order $p = 0$ is simply $F_{0}(x,h) = f(x)$. The first and second order approximations are (3.1) and (3.2) respectively."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The Taylor series sum *converges* if the partial sums converge to $f$:\n",
      "\n",
      "$\\lim\\limits_{p \\to \\infty}F_{p}(x,h) = f(x + h)$ ."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "If there is a positive $h_{0}$ so that the series converges whenever $|h| < h_{0}$, then $f$ is *analytic* at $x$. A function probably is analytic at most points if there is a formula for it, or it is the solution of a differential equation. Figure 3.1 plots a function $f(x)=x2^{2x}$ together with the Taylor approximations of order zero, one, and two. The symbols at the ends of the curves illustrate the convergence of this Taylor series when $h = \\pm .3$. When $h=.3$, the series converges monotonically: $F_{0} < F_{1} < F_{2} \\cdots \\rightarrow f(x+h)$. When $h = -.3$, there are approximants on both sides of the answer: $F_{0} > F_{2} > f(x + h) > F_{1}$."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Footnotes"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "*$^{1}$ In  the notation of Chapter 2, $\\widehat{A}$ is an estimate of the desired answer, A.*\n",
      "\n",
      "*$^{2}$ The nominal order of accuracy may be achieved if $f$ is smooth enough, a point that is important in many applications.*"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}