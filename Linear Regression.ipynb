{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#This will give us matplotlib as plt and numpy as np\n",
      "%pylab inline\n",
      "#This will give us some SymPy basics\n",
      "from sympy import *\n",
      "\n",
      "#Use this to set appropriate sizing for images\n",
      "plt.rc('figure', figsize=(10, 8))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The relationship between variables is said to be *linear* when a \"straight line\" may be drawn using the variables as axes. Many are familiar with the *slope-intercept equation* which I'll present in the form $Y = a + bX$. Traditionally,  $Y$ is said to be the **dependent** variable while $X$ is referred to as the **independent** variable. This is notational, and the equation could be easily rearranged to $X = \\frac{1}{b}Y - \\frac{1}{b}a$ where $X$ is the dependent variable instead. $b$ provides the scalar proportionality of $Y$ to $X$ and is referred to as the *slope* of the line, $a$ is an additive quantity that uniformly shifts the line upwards or downwards depending on its sign. In the case of $X = 0$, the value of $Y$ is equal to $a$ and the point on the line lies on the *y-axis*, thus giving it the name of **y-intercept**."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Some independent variable points\n",
      "x_points = np.arange(-5, 6)\n",
      "#Some dependent variable points\n",
      "y_points_positive = (x_points * 1.0) + 1.0\n",
      "y_points_negative = (x_points * -1.0) + 1.0\n",
      "\n",
      "fig = plt.figure()\n",
      "ax = fig.add_subplot(111)\n",
      "ax.plot(x_points, y_points_positive, linewidth=2, label='positive slope')\n",
      "ax.plot(x_points, y_points_negative, linewidth=2, label='negative slope')\n",
      "ax.set_xlim(-6, 6)\n",
      "ax.set_ylim(-5, 7)\n",
      "ax.set_xlabel('X')\n",
      "ax.set_ylabel('Y')\n",
      "ax.spines['bottom'].set_position('zero')\n",
      "ax.spines['left'].set_position('zero')\n",
      "ax.spines['top'].set_position('zero')\n",
      "ax.spines['right'].set_position('zero')\n",
      "ax.legend(loc='best')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Every value of $Y$ is exactly a function of $X$ in the ideal model of a function. Experimental observations may not match the ideal model due to various sources of error (or if the wrong model is chosen, but that is a unique problem). For now we will not consider error in the measurement of $X$ and instead focus on error in the measurement of $Y$. The following notation is used to describe a regression equation for a linear model\n",
      "\n",
      "$$Y^{\\prime} = a + bX$$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Some independent variable points\n",
      "x_points = np.arange(0, 11)\n",
      "\n",
      "def relative_error(data, rel_err=0.1):\n",
      "    \"\"\"\n",
      "    Applies random error to a data set scaled to the individual data points.\n",
      "    The value passed to rel_err will be a fraction of the datum value. The\n",
      "    default is 0.1, which means that the noise perturbations will fall within\n",
      "    10% of the original value.\n",
      "    \n",
      "    Note: This does not correspond to variance or standard deviation.\n",
      "    \"\"\"\n",
      "    temp_list = []\n",
      "    for datum in data:\n",
      "        sign = [-1.0, 1.0][randint(0,2)]  # Randomly assign + or -\n",
      "        error = sign * random.random() * rel_err * datum\n",
      "        temp_list.append(datum + error)\n",
      "    return np.array(temp_list)\n",
      "\n",
      "#Let's create some y_data points with noise\n",
      "ideal_data = x_points * 1.75 + 0.8  # The \"perfect\" equation\n",
      "noisy_data = relative_error(ideal_data, rel_err=0.2)  # 20% error\n",
      "\n",
      "#NOTE: Since this data has random noise, it will always be a little unique\n",
      "\n",
      "fig = plt.figure()\n",
      "ax = fig.add_subplot(111)\n",
      "ax.plot(x_points, noisy_data, marker='o', linestyle='')\n",
      "ax.plot(x_points, ideal_data, linewidth=2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "So, supposing that a fit for $Y^{\\prime} = ax + b$ was found, each data point has a certain **residual** or **error** which is the difference between the $Y$ value at some $X$.\n",
      "\n",
      "The mean of $Y$ is represented as $\\overline{Y}$ which is calculated by\n",
      "\n",
      "$\\overline{Y} = \\frac{\\sum{Y}}{n}$\n",
      "\n",
      "where $\\sum{Y}$ is the sum of all $Y$-values and $n$ is the quantity of individual values."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#It is simple to find the mean\n",
      "mean_y = np.mean(noisy_data)  # why make it harder?\n",
      "\n",
      "#Let's also get the mean X while we're at it\n",
      "mean_x = np.mean(x_points)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Let's put these on the plot from above\n",
      "\n",
      "fig = plt.figure()\n",
      "ax = fig.add_subplot(111)\n",
      "ax.axhline(mean_y)\n",
      "ax.axvline(mean_x)\n",
      "ax.plot(x_points, noisy_data, marker='o', linestyle='')\n",
      "ax.plot(x_points, ideal_data, linewidth=2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Let's show the deviations from each data point from the mean of y\n",
      "fig = plt.figure()\n",
      "ax = fig.add_subplot(111)\n",
      "ax.axhline(mean_y)\n",
      "ax.axvline(mean_x)\n",
      "ax.plot(x_points, noisy_data, marker='o', linestyle='')\n",
      "#ax.plot(x_points, ideal_data, linewidth=2)\n",
      "\n",
      "ylims = ax.get_ylim()\n",
      "ylen = ylims[1] - ylims[0]\n",
      "\n",
      "for datum, x_val in zip(noisy_data, x_points):\n",
      "    if datum < mean_y:\n",
      "        ymin, ymax = datum, mean_y\n",
      "    else:\n",
      "        ymin, ymax = mean_y, datum\n",
      "    ymin = (ymin - ylims[0]) / ylen\n",
      "    ymax = (ymax - ylims[0]) / ylen\n",
      "    plt.axvline(x_val, ymin=ymin, ymax=ymax, color='r', linestyle='--')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Since we can just copypasta... I'll do the same for the X\n",
      "fig = plt.figure()\n",
      "ax = fig.add_subplot(111)\n",
      "ax.axhline(mean_y)\n",
      "ax.axvline(mean_x)\n",
      "ax.plot(x_points, noisy_data, marker='o', linestyle='')\n",
      "#ax.plot(x_points, ideal_data, linewidth=2)\n",
      "\n",
      "xlims = ax.get_xlim()\n",
      "xlen = xlims[1] - xlims[0]\n",
      "\n",
      "for datum, x_val in zip(noisy_data, x_points):\n",
      "    if datum < mean_y:\n",
      "        ymin, ymax = datum, mean_y\n",
      "    else:\n",
      "        ymin, ymax = mean_y, datum\n",
      "    ymin = (ymin - ylims[0]) / ylen\n",
      "    ymax = (ymax - ylims[0]) / ylen\n",
      "    plt.axvline(x_val, ymin=ymin, ymax=ymax, color='r', linestyle='--')\n",
      "    if x_val < mean_x:\n",
      "        xmin, xmax = x_val, mean_x\n",
      "    else:\n",
      "        xmin, xmax = mean_x, x_val\n",
      "    xmin = (xmin - xlims[0]) / xlen\n",
      "    xmax = (xmax - xlims[0]) / xlen\n",
      "    plt.axhline(datum, xmin=xmin, xmax=xmax, color='r', linestyle='--')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "A good thing to know is that $\\sum(Y - \\overline{Y}) = 0$. This is a basic principle of averages. It's pretty simple to test."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "deviations = noisy_data - mean_y\n",
      "np.sum(deviations)  #This should be *very* small"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can rewrite the above relationship as\n",
      "\n",
      "$\\sum{(Y - \\overline{Y})} = \\sum{Y} - n\\overline{Y}$\n",
      "\n",
      "because the $\\overline{Y}$ is a constant value that is substracted once for each $n$. Above I wrote the definition of $\\overline{Y}$ as\n",
      "\n",
      "$\\overline{Y} = \\frac{\\sum{Y}}{n}$ which can be rearranged to $n\\overline{Y} = \\sum{Y}$. Subsituting in for $n\\overline{Y}$ gives $\\sum{Y} - \\sum{Y} = 0$, proving the basic principle."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now its time to introduce the concept of variance. So I just showed that the sum of the deviations from the mean is $0$. This is not the case for the sum of the *squared* deviation (if the data is at all noisy). Consider the following formula\n",
      "\n",
      "$\\sum{(Y - \\overline{Y})^{2}}$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#We can calculate this value for our noisy data\n",
      "squared_deviations = deviations ** 2\n",
      "np.sum(squared_deviations)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "What would happen if $\\overline{Y}$ were replaced by any other constant value, $c$? In fact, the value will increase for any $c \\neq \\overline{Y}$, the following relationship can be proven\n",
      "\n",
      "$\\sum{(Y - \\overline{Y})^{2}} < \\sum{(Y - c)^{2}}$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "$\\sum{(Y - c)^{2}}$ expands to \n",
      "\n",
      "$\\sum{Y^{2}} - 2c\\sum{Y} + nc^{2}$ which if we differentiate with respect for $c$ and set the equation equal to 0 to find the minimum\n",
      "\n",
      "$-2\\sum{Y} + 2nc = 0$\n",
      "\n",
      "when $c = \\overline{Y}$, we get $2n\\overline{Y} = 2\\sum{Y}$, which is clearly true by the definition $\\overline{Y} = \\frac{\\sum{Y}}{n}$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#I'd like to plot this out for illustration\n",
      "c_points = np.linspace(mean_y-10, mean_y+10, 100)\n",
      "\n",
      "squared_deviation_from_c = []\n",
      "\n",
      "for c_val in c_points:\n",
      "    squared_deviation_from_c.append(np.sum((noisy_data - c_val) ** 2))\n",
      "\n",
      "squared_deviation_from_c = np.array(squared_deviation_from_c)\n",
      "\n",
      "#and I'll put in the derivative, it should become 0 at c=mean_y\n",
      "diff_squared_deviation_from_c = []\n",
      "for c_val in c_points:\n",
      "    diff_squared_deviation_from_c.append(-2.0 * np.sum(noisy_data) + 2.0 * len(noisy_data) * c_val)\n",
      "\n",
      "diff_squared_deviation_from_c = np.array(diff_squared_deviation_from_c)\n",
      "\n",
      "fig = plt.figure()\n",
      "ax1 = fig.add_subplot(211)\n",
      "ax2 = fig.add_subplot(212, sharex=ax1)\n",
      "ax1.plot(c_points, squared_deviation_from_c, label=r'$\\sum{(Y-c)^{2}}$', linewidth=2)\n",
      "ax1.axvline(mean_y, color='r', label=r'$c=\\overline{Y}$')\n",
      "ax1.legend(loc='best')\n",
      "ax1.grid()\n",
      "ax2.axvline(mean_y, color='r', label=r'$c=\\overline{Y}$')\n",
      "ax2.plot(c_points, diff_squared_deviation_from_c, label=r'$\\frac{\\delta}{\\delta c}\\sum{(Y-c){2}}}$', linewidth=2)\n",
      "ax2.legend(loc='best')\n",
      "ax2.grid()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This leads us to introduce the concept of **variance** and **standard deviation** which are represented by $s^{2}$ and $s$ repectively. If the squared deviation from the mean $Y$, $\\overline{Y}$ is divided by $n - 1$, we get the measure known as *variance*.\n",
      "\n",
      "$s^{2} = \\frac{\\sum{(Y - \\overline{Y})^{2}}}{n - 1}$\n",
      "\n",
      "and the square root of this equation gives the *standard deviation*\n",
      "\n",
      "$s = \\sqrt{\\frac{\\sum{(Y - \\overline{Y})^{2}}}{n - 1}}$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#We can calculate these for our data set\n",
      "variance = np.sum(squared_deviations) / (len(noisy_data) - 1)\n",
      "print(variance)\n",
      "standard_deviation = np.sqrt(variance)\n",
      "print(standard_deviation)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "When the variance and standard deviation are small, this indicates that the values of $Y$ are generally very close to the mean. For many random distributions, it is a common principle that 95% of the values in the data set will exist within *2s* of the mean."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Linear Regression of Data"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's return to the equation\n",
      "\n",
      "$Y^{\\prime} = a + bx$\n",
      "\n",
      "which represents a regression (or fit) line for a set of data in $X$ and $Y$. $a$ may be expressed as\n",
      "\n",
      "$a = \\overline{Y} - b \\overline{X}$\n",
      "\n",
      "and $b$ may be expressed as\n",
      "\n",
      "$b = \\frac{\\sum{((X-\\overline{X})}(Y-\\overline{Y}))}{\\sum{(X-\\overline{X})^{2}}}$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Earlier I mentioned residuals as the difference between the ideal function of $Y(x)$, which we are referring to here as $Y^{\\prime}$, and the data itself represented by $Y$. Keep in mind that our working assumption is that the data *does* adhere to our ideal mathematical expression, but other factors are introducing noise. Here is a formula for the residual\n",
      "\n",
      "$Y - Y^{\\prime} = Y - (a + bX) = e$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Let's use the ideal data as our Y', recall that its formula\n",
      "#above is Y' = 1.75 * X + 0.8\n",
      "residuals = noisy_data - ideal_data\n",
      "print(residuals)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Least Squares Fitting"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "According to this method of fitting, the best fit of a function to the data will *minimize the sum  of  squared residuals*. We can write the formula for the sum of squared residuals as\n",
      "\n",
      "$\\sum{e^{2}} = \\sum{(Y - Y^{\\prime})^{2}} = \\sum{(Y - (a + bX))^{2}}$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can show that the values for $a$ and $b$ must satisfy the following two equations for minimizing conditions in *least squares*\n",
      "\n",
      "$\\sum{Y} = na + b\\sum{X}$\n",
      "\n",
      "and\n",
      "\n",
      "$\\sum{XY} = a\\sum{X} + b\\sum{X^{2}}$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "So if we expand the formula for the squared residuals we will get the following expression\n",
      "\n",
      "$\\sum{Y^{2}} - 2a\\sum{Y} -2b\\sum{XY} + na^{2} + 2ab\\sum{X} + b^{2}\\sum{X^{2}}$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The derivative of this expression with respect to $a$ is\n",
      "\n",
      "$-2\\sum{Y} + 2na + 2b\\sum{X}$\n",
      "\n",
      "and the derivative of this expression with respect to $b$ is\n",
      "\n",
      "$-2\\sum{XY} + 2a\\sum{X} + 2b\\sum{X^{2}}$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "By setting them equal to 0, we will be solving for a minimum in the function $F(a,b) = \\sum{e^{2}} = \\sum{(Y - (a + bX))^{2}}$ (we know it's a minimum because it's a positive square function for both $a$ and $b$)."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "So from \n",
      "\n",
      "$-2\\sum{Y} + 2na + 2b\\sum{X} = 0$ \n",
      "\n",
      "$-2\\sum{XY} + 2a\\sum{X} + 2b\\sum{X^{2}} = 0$ \n",
      "\n",
      "we get\n",
      "\n",
      "$\\sum{Y} = na + b\\sum{X}$\n",
      "\n",
      "$\\sum{XY} = a\\sum{X} + b\\sum{X^{2}}$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "These may be rearranged algebraically (along with the definition of means $\\overline{Y} = \\frac{\\sum{Y}}{n}$ and $\\overline{X} = \\frac{\\sum{X}}{n}$ to yield the following simplified expressions for $a$ and $b$. You can either trust me on this or work it out (it's easy).\n",
      "\n",
      "$a = \\overline{Y} - b\\overline{X}$\n",
      "\n",
      "$b = \\frac{\\sum{XY} - n\\overline{X}\\overline{Y}}{\\sum{X^{2}} - n\\overline{X}^{2}} = \\frac{\\sum{XY} - (\\sum{X})(\\sum{Y})/n}{\\sum{X^{2}} - (\\sum{X})^{2}/n}$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Ultimately this gives an analytic solution to the minimization of least squares, a function which finally depends solely on $b$ since this function is *anchored* (must pass through) to the point $(\\overline{X}, \\overline{Y})$. This is specifically a solution for *linear* fitting; general least squares fitting will employ other methods."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#So let's finally put this solution in action for our data\n",
      "sum_xy = np.sum(x_points * noisy_data)\n",
      "sum_x = np.sum(x_points)\n",
      "sum_x_squared = np.sum(x_points ** 2)\n",
      "sum_y = np.sum(noisy_data)\n",
      "n = len(x_points)\n",
      "\n",
      "b = (sum_xy - sum_x * sum_y / n) / (sum_x_squared - sum_x ** 2 / n)\n",
      "a = mean_y - b * mean_x\n",
      "\n",
      "fig = plt.figure()\n",
      "ax = fig.add_subplot(111)\n",
      "ax.plot(x_points, noisy_data, marker='o', color='b', linestyle='')\n",
      "ax.plot(x_points, ideal_data, linewidth=2, color='g', label='ideal without noise', linestyle='--')\n",
      "ax.plot(x_points, map(lambda x: a + b * x, x_points), linewidth=2, label='best fit')\n",
      "ax.axvline(mean_x, label=r'$\\overline{X}$', color = 'r')\n",
      "ax.axhline(mean_y, label=r'$\\overline{Y}$', color = 'r')\n",
      "ax.grid()\n",
      "ax.legend(loc='best')\n",
      "ax.set_xlabel('X')\n",
      "ax.set_xlabel('Y')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}