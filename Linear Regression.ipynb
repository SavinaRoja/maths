{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#This will give us matplotlib as plt and numpy as np\n",
      "%pylab inline\n",
      "#This will give us some SymPy basics\n",
      "from sympy import *\n",
      "\n",
      "#Use this to set appropriate sizing for images\n",
      "plt.rc('figure', figsize=(10, 8))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The relationship between variables is said to be *linear* when a \"straight line\" may be drawn using the variables as axes. Many are familiar with the *slope-intercept equation* which I'll present in the form $Y = a + bX$. Traditionally,  $Y$ is said to be the **dependent** variable while $X$ is referred to as the **independent** variable. This is notational, and the equation could be easily rearranged to $X = \\frac{1}{b}Y - \\frac{1}{b}a$ where $X$ is the dependent variable instead. $b$ provides the scalar proportionality of $Y$ to $X$ and is referred to as the *slope* of the line, $a$ is an additive quantity that uniformly shifts the line upwards or downwards depending on its sign. In the case of $X = 0$, the value of $Y$ is equal to $a$ and the point on the line lies on the *y-axis*, thus giving it the name of **y-intercept**."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Some independent variable points\n",
      "x_points = np.arange(-5, 6)\n",
      "#Some dependent variable points\n",
      "y_points_positive = (x_points * 1.0) + 1.0\n",
      "y_points_negative = (x_points * -1.0) + 1.0\n",
      "\n",
      "fig = plt.figure()\n",
      "ax = fig.add_subplot(111)\n",
      "\n",
      "ax.plot(x_points, y_points_positive, linewidth=2, label='positive slope')\n",
      "ax.plot(x_points, y_points_negative, linewidth=2, label='negative slope')\n",
      "ax.set_xlim(-6, 6)\n",
      "ax.set_ylim(-5, 7)\n",
      "ax.set_xlabel('X')\n",
      "ax.set_ylabel('Y')\n",
      "ax.spines['bottom'].set_position('zero')\n",
      "ax.spines['left'].set_position('zero')\n",
      "ax.spines['top'].set_position('zero')\n",
      "ax.spines['right'].set_position('zero')\n",
      "ax.legend(loc='best')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Every value of $Y$ is exactly a function of $X$ in the ideal model of a function. Experimental observations may not match the ideal model due to various sources of error (or if the wrong model is chosen, but that is a unique problem). For now we will not consider error in the measurement of $X$ and instead focus on error in the measurement of $Y$. The following notation is used to describe a regression equation for a linear model\n",
      "\n",
      "$$Y^{\\prime} = a + bX$$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Some independent variable points\n",
      "x_points = np.arange(0, 11)\n",
      "\n",
      "def relative_error(data, rel_err=0.1):\n",
      "    \"\"\"\n",
      "    Applies random error to a data set scaled to the individual data points.\n",
      "    The value passed to rel_err will be a fraction of the datum value. The\n",
      "    default is 0.1, which means that the noise perturbations will fall within\n",
      "    10% of the original value.\n",
      "    \n",
      "    Note: This does not correspond to variance or standard deviation.\n",
      "    \"\"\"\n",
      "    temp_list = []\n",
      "    for datum in data:\n",
      "        sign = [-1.0, 1.0][randint(0,2)]  # Randomly assign + or -\n",
      "        error = sign * random.random() * rel_err * datum\n",
      "        temp_list.append(datum + error)\n",
      "    return np.array(temp_list)\n",
      "\n",
      "#Let's create some y_data points with noise\n",
      "ideal_data = x_points * 1.75 + 0.8  # The \"perfect\" equation\n",
      "noisy_data = relative_error(ideal_data, rel_err=0.2)  # 20% error\n",
      "\n",
      "fig = plt.figure()\n",
      "ax = fig.add_subplot(111)\n",
      "ax.plot(x_points, noisy_data, marker='o', linestyle='')\n",
      "ax.plot(x_points, ideal_data, linewidth=2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "So, supposing that a fit for $Y^{\\prime} = ax + b$ was found, each data point has a certain **residual** or **error** which is the difference between the $Y$ value at some $X$.\n",
      "\n",
      "The mean of $Y$ is represented as $\\overline{Y}$ which is calculated by\n",
      "\n",
      "$\\overline{Y} = \\frac{\\sum{Y}}{n}$\n",
      "\n",
      "where $\\sum{Y}$ is the sum of all $Y$-values and $n$ is the quantity of individual values."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#It is simple to find the mean\n",
      "mean_y = np.mean(noisy_data)  # why make it harder?\n",
      "\n",
      "#Let's also get the mean X while we're at it\n",
      "mean_x = np.mean(x_points)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Let's put these on the plot from above\n",
      "\n",
      "fig = plt.figure()\n",
      "ax = fig.add_subplot(111)\n",
      "ax.axhline(mean_y)\n",
      "ax.axvline(mean_x)\n",
      "ax.plot(x_points, noisy_data, marker='o', linestyle='')\n",
      "ax.plot(x_points, ideal_data, linewidth=2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Let's show the deviations from each data point from the mean of y\n",
      "fig = plt.figure()\n",
      "ax = fig.add_subplot(111)\n",
      "ax.axhline(mean_y)\n",
      "ax.axvline(mean_x)\n",
      "ax.plot(x_points, noisy_data, marker='o', linestyle='')\n",
      "#ax.plot(x_points, ideal_data, linewidth=2)\n",
      "\n",
      "ylims = ax.get_ylim()\n",
      "ylen = ylims[1] - ylims[0]\n",
      "\n",
      "for datum, x_val in zip(noisy_data, x_points):\n",
      "    if datum < mean_y:\n",
      "        ymin, ymax = datum, mean_y\n",
      "    else:\n",
      "        ymin, ymax = mean_y, datum\n",
      "    ymin = (ymin - ylims[0]) / ylen\n",
      "    ymax = (ymax - ylims[0]) / ylen\n",
      "    plt.axvline(x_val, ymin=ymin, ymax=ymax, color='r', linestyle='--')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Since we can just copypasta... I'll do the same for the X\n",
      "fig = plt.figure()\n",
      "ax = fig.add_subplot(111)\n",
      "ax.axhline(mean_y)\n",
      "ax.axvline(mean_x)\n",
      "ax.plot(x_points, noisy_data, marker='o', linestyle='')\n",
      "#ax.plot(x_points, ideal_data, linewidth=2)\n",
      "\n",
      "xlims = ax.get_xlim()\n",
      "xlen = xlims[1] - xlims[0]\n",
      "\n",
      "for datum, x_val in zip(noisy_data, x_points):\n",
      "    if datum < mean_y:\n",
      "        ymin, ymax = datum, mean_y\n",
      "    else:\n",
      "        ymin, ymax = mean_y, datum\n",
      "    ymin = (ymin - ylims[0]) / ylen\n",
      "    ymax = (ymax - ylims[0]) / ylen\n",
      "    plt.axvline(x_val, ymin=ymin, ymax=ymax, color='r', linestyle='--')\n",
      "    if x_val < mean_x:\n",
      "        xmin, xmax = x_val, mean_x\n",
      "    else:\n",
      "        xmin, xmax = mean_x, x_val\n",
      "    xmin = (xmin - xlims[0]) / xlen\n",
      "    xmax = (xmax - xlims[0]) / xlen\n",
      "    plt.axhline(datum, xmin=xmin, xmax=xmax, color='r', linestyle='--')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "A good thing to know is that $\\sum(Y - \\overline{Y}) = 0$. This is a basic principle of averages. It's pretty simple to test."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "deviations = noisy_data - mean_y\n",
      "np.sum(deviations)  #This should be *very* small"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can rewrite the above relationship as\n",
      "\n",
      "$\\sum{(Y - \\overline{Y})} = \\sum{Y} - n\\overline{Y}$\n",
      "\n",
      "because the $\\overline{Y}$ is a constant value that is substracted once for each $n$. Above I wrote the definition of $\\overline{Y}$ as\n",
      "\n",
      "$\\overline{Y} = \\frac{\\sum{Y}}{n}$ which can be rearranged to $n\\overline{Y} = \\sum{Y}$. Subsituting in for $n\\overline{Y}$ gives $\\sum{Y} - \\sum{Y} = 0$, proving the basic principle."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now its time to introduce the concept of variance. So I just showed that the sum of the deviations from the mean is $0$. This is not the case for the sum of the *squared* deviation (if the data is at all noisy). Consider the following formula\n",
      "\n",
      "$\\sum{(Y - \\overline{Y})^{2}}$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#We can calculate this value for our noisy data\n",
      "squared_deviations = deviations ** 2\n",
      "np.sum(squared_deviations)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "What would happen if $\\overline{Y}$ were replaced by any other constant value, $c$? In fact, the value will increase for any $c \\neq \\overline{Y}$, the following relationship can be proven\n",
      "\n",
      "$\\sum{(Y - \\overline{Y})^{2}} < \\sum{(Y - c)^{2}}$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "$\\sum{(Y - c)^{2}}$ expands to \n",
      "\n",
      "$\\sum{Y^{2}} - 2c\\sum{Y} + nc^{2}$ which if we differentiate with respect for $c$ and set the equation equal to 0 to find the minimum\n",
      "\n",
      "$-2\\sum{Y} + 2nc = 0$\n",
      "\n",
      "when $c = \\overline{Y}$, we get $2n\\overline{Y} = 2\\sum{Y}$, which is clearly true by the definition $\\overline{Y} = \\frac{\\sum{Y}}{n}$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#I'd like to plot this out for illustration\n",
      "c_points = np.linspace(mean_y-10, mean_y+10, 100)\n",
      "\n",
      "squared_deviation_from_c = []\n",
      "\n",
      "for c_val in c_points:\n",
      "    squared_deviation_from_c.append(np.sum((noisy_data - c_val) ** 2))\n",
      "\n",
      "squared_deviation_from_c = np.array(squared_deviation_from_c)\n",
      "\n",
      "#and I'll put in the derivative, it should become 0 at c=mean_y\n",
      "diff_squared_deviation_from_c = []\n",
      "for c_val in c_points:\n",
      "    diff_squared_deviation_from_c.append(-2.0 * np.sum(noisy_data) + 2.0 * len(noisy_data) * c_val)\n",
      "\n",
      "diff_squared_deviation_from_c = np.array(diff_squared_deviation_from_c)\n",
      "    \n",
      "\n",
      "fig = plt.figure()\n",
      "ax1 = fig.add_subplot(211)\n",
      "ax2 = fig.add_subplot(212, sharex=ax1)\n",
      "ax1.plot(c_points, squared_deviation_from_c, label=r'$\\sum{(Y-c)^{2}}$', linewidth=2)\n",
      "ax1.axvline(mean_y, color='r', label=r'$c=\\overline{Y}$')\n",
      "ax1.legend(loc='best')\n",
      "ax1.grid()\n",
      "ax2.axvline(mean_y, color='r', label=r'$c=\\overline{Y}$')\n",
      "ax2.plot(c_points, diff_squared_deviation_from_c, label=r'$\\frac{\\delta}{\\delta c}\\sum{(Y-c){2}}}$', linewidth=2)\n",
      "ax2.legend(loc='best')\n",
      "ax2.grid()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This leads us to introduce the concept of **variance** and **standard deviation** which are represented by $s^{2}$ and $s$ repectively. If the squared deviation from the mean $Y$, $\\overline{Y}$ is divided by $n - 1$, we get the measure known as *variance*.\n",
      "\n",
      "$s^{2} = \\frac{\\sum{(Y - \\overline{Y})^{2}}}{n - 1}$\n",
      "\n",
      "and the square root of this equation gives the *standard deviation*\n",
      "\n",
      "$s = \\sqrt{\\frac{\\sum{(Y - \\overline{Y})^{2}}}{n - 1}}$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#We can calculate these for our data set\n",
      "variance = np.sum(squared_deviations) / (len(noisy_data) - 1)\n",
      "print(variance)\n",
      "standard_deviation = np.sqrt(variance)\n",
      "print(standard_deviation)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "When the variance and standard deviation are small, this indicates that the values of $Y$ are generally very close to the mean. For many random distributions, it is a common principle that 95% of the values in the data set will exist within *2s* of the mean."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Linear Regression of Data"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's return to the equation\n",
      "\n",
      "$Y^{\\prime} = a + bx$\n",
      "\n",
      "which represents a regression (or fit) line for a set of data in $X$ and $Y$. $a$ may be expressed as\n",
      "\n",
      "$a = \\overline{Y} - b \\overline{X}$\n",
      "\n",
      "and $b$ may be expressed as\n",
      "\n",
      "$b = \\frac{\\sum{((X-\\overline{X})}(Y-\\overline{Y}))}{\\sum{(X-\\overline{X})^{2}}}$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Earlier I mentioned residuals as the difference between the ideal function of $Y(x)$, which we are referring to here as $Y^{\\prime}$, and the data itself represented by $Y$. Keep in mind that our working assumption is that the data *does* adhere to our ideal mathematical expression, but other factors are introducing noise."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}