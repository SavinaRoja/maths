{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#This will give us matplotlib as plt and numpy as np\n",
      "%pylab inline\n",
      "#This will give us some SymPy basics\n",
      "from sympy import *\n",
      "\n",
      "#Use this to set appropriate sizing for images\n",
      "plt.rc('figure', figsize=(10, 8))\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The relationship between variables is said to be *linear* when a \"straight line\" may be drawn using the variables as axes. Many are familiar with the *slope-intercept equation* which I'll present in the form $Y = a + bX$. Traditionally,  $Y$ is said to be the **dependent** variable while $X$ is referred to as the **independent** variable. This is notational, and the equation could be easily rearranged to $X = \\frac{1}{b}Y - \\frac{1}{b}a$ where $X$ is the dependent variable instead. $b$ provides the scalar proportionality of $Y$ to $X$ and is referred to as the *slope* of the line, $a$ is an additive quantity that uniformly shifts the line upwards or downwards depending on its sign. In the case of $X = 0$, the value of $Y$ is equal to $a$ and the point on the line lies on the *y-axis*, thus giving it the name of **y-intercept**."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Some independent variable points\n",
      "x_points = np.arange(-5, 6)\n",
      "#Some dependent variable points\n",
      "y_points_positive = (x_points * 1.0) + 1.0\n",
      "y_points_negative = (x_points * -1.0) + 1.0\n",
      "\n",
      "fig = plt.figure()\n",
      "ax = fig.add_subplot(111)\n",
      "\n",
      "ax.plot(x_points, y_points_positive, linewidth=2, label='positive slope')\n",
      "ax.plot(x_points, y_points_negative, linewidth=2, label='negative slope')\n",
      "ax.set_xlim(-6, 6)\n",
      "ax.set_ylim(-5, 7)\n",
      "ax.set_xlabel('X')\n",
      "ax.set_ylabel('Y')\n",
      "ax.spines['bottom'].set_position('zero')\n",
      "ax.spines['left'].set_position('zero')\n",
      "ax.spines['top'].set_position('zero')\n",
      "ax.spines['right'].set_position('zero')\n",
      "ax.legend(loc='best')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Every value of $Y$ is exactly a function of $X$ in the ideal model of a function. Experimental observations may not match the ideal model due to various sources of error (or if the wrong model is chosen, but that is a unique problem). For now we will not consider error in the measurement of $X$ and instead focus on error in the measurement of $Y$. The following notation is used to describe a regression equation for a linear model\n",
      "\n",
      "$$Y^{\\prime} = a + bX$$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Some independent variable points\n",
      "x_points = np.arange(0, 11)\n",
      "\n",
      "def relative_error(data, rel_err=0.1):\n",
      "    \"\"\"\n",
      "    Applies random error to a data set scaled to the individual data points.\n",
      "    The value passed to rel_err will be a fraction of the datum value. The\n",
      "    default is 0.1, which means that the noise perturbations will fall within\n",
      "    10% of the original value.\n",
      "    \n",
      "    Note: This does not correspond to variance or standard deviation.\n",
      "    \"\"\"\n",
      "    temp_list = []\n",
      "    for datum in data:\n",
      "        sign = [-1.0, 1.0][randint(0,2)]  # Randomly assign + or -\n",
      "        error = sign * random.random() * rel_err * datum\n",
      "        temp_list.append(datum + error)\n",
      "    return np.array(temp_list)\n",
      "\n",
      "#Let's create some y_data points with noise\n",
      "ideal_data = x_points * 1.75 + 0.8  # The \"perfect\" equation\n",
      "noisy_data = relative_error(ideal_data, rel_err=0.2)  # 20% error\n",
      "\n",
      "fig = plt.figure()\n",
      "ax = fig.add_subplot(111)\n",
      "ax.plot(x_points, noisy_data, marker='o', linestyle='')\n",
      "ax.plot(x_points, ideal_data, linewidth=2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "So, supposing that a fit for $Y^{\\prime} = ax + b$ was found, each data point has a certain **residual** or **error** which is the difference between the $Y$ value at some $X$.\n",
      "\n",
      "The mean of $Y$ is represented as $\\overline{Y}$ which is calculated by\n",
      "\n",
      "$\\overline{Y} = \\frac{\\sum{Y}}{n}$\n",
      "\n",
      "where $\\sum{Y}$ is the sum of all $Y$-values and $n$ is the quantity of individual values."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#It is simple to find the mean\n",
      "mean_y = np.mean(noisy_data)  # why make it harder?\n",
      "\n",
      "#Let's also get the mean X while we're at it\n",
      "mean_x = np.mean(x_points)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Let's put these on the plot from above\n",
      "\n",
      "fig = plt.figure()\n",
      "ax = fig.add_subplot(111)\n",
      "ax.axhline(mean_y)\n",
      "ax.axvline(mean_x)\n",
      "ax.plot(x_points, noisy_data, marker='o', linestyle='')\n",
      "ax.plot(x_points, ideal_data, linewidth=2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Let's show the deviations from each data point from the mean of y\n",
      "fig = plt.figure()\n",
      "ax = fig.add_subplot(111)\n",
      "ax.axhline(mean_y)\n",
      "ax.axvline(mean_x)\n",
      "ax.plot(x_points, noisy_data, marker='o', linestyle='')\n",
      "#ax.plot(x_points, ideal_data, linewidth=2)\n",
      "\n",
      "ylims = ax.get_ylim()\n",
      "ylen = ylims[1] - ylims[0]\n",
      "\n",
      "for datum, x_val in zip(noisy_data, x_points):\n",
      "    if datum < mean_y:\n",
      "        ymin, ymax = datum, mean_y\n",
      "    else:\n",
      "        ymin, ymax = mean_y, datum\n",
      "    ymin = (ymin - ylims[0]) / ylen\n",
      "    ymax = (ymax - ylims[0]) / ylen\n",
      "    plt.axvline(x_val, ymin=ymin, ymax=ymax, color='r', linestyle='--')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Since we can just copypasta... I'll do the same for the X\n",
      "fig = plt.figure()\n",
      "ax = fig.add_subplot(111)\n",
      "ax.axhline(mean_y)\n",
      "ax.axvline(mean_x)\n",
      "ax.plot(x_points, noisy_data, marker='o', linestyle='')\n",
      "#ax.plot(x_points, ideal_data, linewidth=2)\n",
      "\n",
      "xlims = ax.get_xlim()\n",
      "xlen = xlims[1] - xlims[0]\n",
      "\n",
      "for datum, x_val in zip(noisy_data, x_points):\n",
      "    if datum < mean_y:\n",
      "        ymin, ymax = datum, mean_y\n",
      "    else:\n",
      "        ymin, ymax = mean_y, datum\n",
      "    ymin = (ymin - ylims[0]) / ylen\n",
      "    ymax = (ymax - ylims[0]) / ylen\n",
      "    plt.axvline(x_val, ymin=ymin, ymax=ymax, color='r', linestyle='--')\n",
      "    if x_val < mean_x:\n",
      "        xmin, xmax = x_val, mean_x\n",
      "    else:\n",
      "        xmin, xmax = mean_x, x_val\n",
      "    xmin = (xmin - xlims[0]) / xlen\n",
      "    xmax = (xmax - xlims[0]) / xlen\n",
      "    plt.axhline(datum, xmin=xmin, xmax=xmax, color='r', linestyle='--')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "A good thing to know is that $\\sum(Y - \\overline{Y}) = 0$. This is a basic principle of averages. It's pretty simple to test."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "deviations = noisy_data - mean_y\n",
      "np.sum(deviations)  #This should be *very* small"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can rewrite the above relationship as\n",
      "\n",
      "$\\sum{Y - \\overline{Y}} = \\sum{Y} - n\\overline{Y}$\n",
      "\n",
      "because the $\\overline{Y}$ is a constant value that is substracted once for each $n$. Above I wrote the definition of $\\overline{Y}$ as\n",
      "\n",
      "$\\overline{Y} = \\frac{\\sum{Y}}{n}$ which can be rearranged to $n\\overline{Y} = \\sum{Y}$. Subsituting in for $n\\overline{Y}$ gives $\\sum{Y} - \\sum{Y} = 0$, proving the basic principle."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now its time to introduce the concept of variance. So I just showed that the sum of the deviations from the mean is $0$. This is not the case for the sum of the *squared* deviation (if the data is at all noisy). Consider the following formula\n",
      "\n",
      "$\\sum{(Y - \\overline{Y})^{2}}$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#We can calculate this value for our noisy data\n",
      "squared_deviations = deviations ** 2\n",
      "np.sum(squared_deviations)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}